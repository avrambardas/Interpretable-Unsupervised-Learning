{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantitative Experiments with LXDR\n",
    "In this notebook, we are performing quantitative experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y0ILeXGuL2kb"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import site, pprint\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, maxabs_scale\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.metrics.pairwise import cosine_similarity as cd, euclidean_distances as ed\n",
    "from tensorflow.keras import layers, losses, callbacks, Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from lxdr import LXDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need PCA, KPCA and AE as DR techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(Model):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim   \n",
    "        self.encoder = Sequential([\n",
    "          layers.Dense(int(input_dim/2)+2, activation='tanh'),\n",
    "          layers.Dense(int(input_dim/2)+1, activation='tanh'),\n",
    "          layers.Dense(latent_dim, activation='tanh'),\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "          layers.Dense(int(input_dim/2)+1, activation='tanh'),\n",
    "          layers.Dense(int(input_dim/2)+2, activation='tanh'),\n",
    "          layers.Dense(input_dim, activation='tanh'),\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 3 real datasets, and later few synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xk4jqCP5o80u"
   },
   "outputs": [],
   "source": [
    "#datasets\n",
    "dataset_names = ['Iris', 'Diabetes', 'Digits']\n",
    "iris = datasets.load_iris()\n",
    "iris_data = iris.data\n",
    "iris_predictions = iris.target\n",
    "iris_dimensions = 3\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_data = diabetes.data\n",
    "diabetes_predictions = diabetes.target\n",
    "diabetes_dimensions = 8\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "digits_data = digits.data\n",
    "digits_predictions = digits.target\n",
    "digits_dimensions = 25\n",
    "\n",
    "sample = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare our data! And we train the different DR techniques!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'X':[iris_data, diabetes_data, digits_data], \n",
    "        'y':[iris_predictions, diabetes_predictions, digits_predictions],\n",
    "        'd':[iris_dimensions, diabetes_dimensions, digits_dimensions],\n",
    "        'n':[50, 150, 750],\n",
    "        'scaler':[]}\n",
    "model = {'PCA':[], 'KPCA':[], 'AE':[]}\n",
    "\n",
    "for X, y, d in zip(data['X'],data['y'],data['d']):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)   \n",
    "    scaler = MaxAbsScaler().fit(x_train)\n",
    "    data['scaler'].append(scaler)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    pca = PCA(d, random_state=42)\n",
    "    pca.fit(x_train, y_train)\n",
    "    model['PCA'].append(pca)\n",
    "    \n",
    "    kpca = KernelPCA(d, kernel='rbf', random_state=42)\n",
    "    kpca.fit(x_train, y_train)\n",
    "    kpca.fit(x_train, y_train)\n",
    "    model['KPCA'].append(kpca)\n",
    "    \n",
    "    callback = callbacks.EarlyStopping(monitor='loss', patience=3, verbose=0, restore_best_weights=True)\n",
    "    autoencoder = Autoencoder(len(x_train[0]),d)\n",
    "    autoencoder.compile(optimizer='adam', loss='mae')\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                    epochs=200,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[callback],\n",
    "                    verbose=0)\n",
    "    model['AE'].append(autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to measure the weights difference and the instance difference as presented in our paper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('results_real_datasets.csv','w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    dataset = 0\n",
    "    for X, y, d, n in zip(data['X'],data['y'],data['d'],data['n']):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "        feature_names = ['F'+str(i) for i in range(1,len(x_train[0])+1)]\n",
    "        scaler = data['scaler'][dataset]\n",
    "        x_train = scaler.transform(x_train)\n",
    "        mean = x_train.mean(axis=0)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        pca = model['PCA'][dataset]\n",
    "        x_test_pca = pca.transform(x_test)\n",
    "\n",
    "        kpca = model['KPCA'][dataset]\n",
    "        x_test_kpca = kpca.transform(x_test)\n",
    "\n",
    "        ae = model['AE'][dataset]\n",
    "        x_test_ae = ae.predict(x_test)  \n",
    "        \n",
    "        for scope in ['local','global']:\n",
    "            lxdr_pca =  LXDR(pca, feature_names, scope, x_train, mean=mean)\n",
    "            lxdr_kpca =  LXDR(kpca, feature_names, scope, x_train)\n",
    "            lxdr_ae =  LXDR(ae, feature_names, scope, x_train, True)\n",
    "\n",
    "            ed_pca = []\n",
    "            cd_pca = []\n",
    "            mae_pca = []\n",
    "\n",
    "            ed_kpca = []\n",
    "            cd_kpca = []\n",
    "            mae_kpca = []\n",
    "\n",
    "            ed_ae = []\n",
    "            cd_ae = []\n",
    "            mae_ae = []\n",
    "\n",
    "            instance = 0\n",
    "            for x in x_test:\n",
    "                weights_pca = lxdr_pca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                a = pca.components_.reshape((1,-1))[0]\n",
    "                b = np.array(weights_pca).reshape((1,-1))[0]\n",
    "\n",
    "                ed_pca.append(ed([a],[b])[0][0])\n",
    "                cd_pca.append(1-cd([a],[b])[0][0])\n",
    "                mae_pca.append(mae([a],[b]))\n",
    "\n",
    "                weights_kpca = lxdr_kpca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                ldrx_instance = np.dot(x, weights_kpca.T)\n",
    "                a = x_test_kpca[instance]\n",
    "                b = ldrx_instance\n",
    "\n",
    "                ed_kpca.append(ed([a],[b])[0][0])\n",
    "                cd_kpca.append(1-cd([a],[b])[0][0])\n",
    "                mae_kpca.append(mae([a],[b]))\n",
    "\n",
    "                weights_ae = lxdr_ae.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                ldrx_instance = np.dot(x, weights_ae.T)\n",
    "                a = x_test_ae[instance]\n",
    "                b = ldrx_instance\n",
    "\n",
    "                ed_ae.append(ed([a],[b])[0][0])\n",
    "                cd_ae.append(1-cd([a],[b])[0][0])\n",
    "                mae_ae.append(mae([a],[b]))\n",
    "\n",
    "                instance += 1\n",
    "                \n",
    "            writer.writerow([dataset_names[dataset],'pca',scope,\n",
    "                                         sum(ed_pca)/len(x_test), \n",
    "                                         sum(cd_pca)/len(x_test), \n",
    "                                         sum(mae_pca)/len(x_test)])\n",
    "            writer.writerow([dataset_names[dataset],'kpca',scope,\n",
    "                                          sum(ed_kpca)/len(x_test), \n",
    "                                          sum(cd_kpca)/len(x_test), \n",
    "                                          sum(mae_kpca)/len(x_test)])\n",
    "            writer.writerow([dataset_names[dataset],'ae',scope, \n",
    "                                        sum(ed_ae)/len(x_test), \n",
    "                                        sum(cd_ae)/len(x_test), \n",
    "                                        sum(mae_ae)/len(x_test)])\n",
    "        dataset += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also experiment with 25 synthetically created datasets to measure the aforementioned metrics, as well as to perform a scalability analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import csv\n",
    "with open('scalability.csv','w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    for features in range(10,260,10):\n",
    "        feature_names = ['F'+str(i) for i in range(features)]\n",
    "        #We increase here the features of a dataset!\n",
    "        x, y = make_classification(n_samples=1000, n_features=features, n_informative=int(features/2), n_redundant=int(features/2), n_classes=2, shuffle=True, random_state=1)\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42) \n",
    "        scaler = MaxAbsScaler().fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        mean = x_train.mean(axis=0)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        pca = PCA(int(features/2), random_state=42)\n",
    "        pca.fit(x_train, y_train)\n",
    "        x_test_pca = pca.transform(x_test)\n",
    "        \n",
    "        kpca = KernelPCA(int(features/2), random_state=42)\n",
    "        kpca.fit(x_train, y_train)\n",
    "        x_test_kpca = kpca.transform(x_test)\n",
    "        \n",
    "        for scope in ['global']:\n",
    "            n = 0\n",
    "            \n",
    "            lxdr_pca =  LXDR(pca, feature_names, scope, x_train, mean=mean)\n",
    "            lxdr_kpca =  LXDR(kpca, feature_names, scope, x_train)\n",
    "\n",
    "            ed_pca = []\n",
    "            cd_pca = []\n",
    "            mae_pca = []\n",
    "            \n",
    "            ed_kpca = []\n",
    "            cd_kpca = []\n",
    "            time_kpca = []\n",
    "\n",
    "            instance = 0\n",
    "            ts = time.time()\n",
    "            for x in x_test[:5]:\n",
    "                weights_pca = lxdr_pca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                a = pca.components_.reshape((1,-1))[0]\n",
    "                b = np.array(weights_pca).reshape((1,-1))[0]\n",
    "\n",
    "                ed_pca.append(ed([a],[b])[0][0])\n",
    "                cd_pca.append(1-cd([a],[b])[0][0])\n",
    "                mae_pca.append(mae([a],[b]))\n",
    "\n",
    "                weights_kpca = lxdr_kpca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                ldrx_instance = np.dot(x, weights_kpca.T)\n",
    "                a = x_test_kpca[instance]\n",
    "                b = ldrx_instance\n",
    "\n",
    "                ed_kpca.append(ed([a],[b])[0][0])\n",
    "                cd_kpca.append(1-cd([a],[b])[0][0])\n",
    "\n",
    "            writer.writerow([features,'kpca',scope,n,\n",
    "                                         sum(ed_kpca)/5, \n",
    "                                         sum(cd_kpca)/5, \n",
    "                                         (time.time()-ts)/5])\n",
    "\n",
    "                \n",
    "            writer.writerow([features,'pca',scope,n,\n",
    "                                         sum(ed_pca)/5, \n",
    "                                         sum(cd_pca)/5, \n",
    "                                         sum(mae_pca)/5])\n",
    "        for scope in ['local']:\n",
    "            for n in [int(1000/4),int(2*1000/4),int(3*1000/4)]:\n",
    "                lxdr_pca =  DRX(pca, feature_names, scope, x_train, mean=mean)\n",
    "                lxdr_kpca =  DRX(kpca, feature_names, scope, x_train)\n",
    "\n",
    "                ed_pca = []\n",
    "                cd_pca = []\n",
    "                mae_pca = []\n",
    "                \n",
    "                ed_kpca = []\n",
    "                cd_kpca = []\n",
    "                time_kpca = []\n",
    "                \n",
    "                instance = 0\n",
    "                ts = time.time()\n",
    "                for x in x_test[:5]:\n",
    "\n",
    "                    weights_pca = lxdr_pca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                    a = pca.components_.reshape((1,-1))[0]\n",
    "                    b = np.array(weights_pca).reshape((1,-1))[0]\n",
    "\n",
    "                    ed_pca.append(ed([a],[b])[0][0])\n",
    "                    cd_pca.append(1-cd([a],[b])[0][0])\n",
    "                    mae_pca.append(mae([a],[b]))\n",
    "                    \n",
    "                    weights_kpca = lxdr_kpca.explain_instance(x, n, auto_alpha=True, use_LIME=False)\n",
    "                    ldrx_instance = np.dot(x, weights_kpca.T)\n",
    "                    a = x_test_kpca[instance]\n",
    "                    b = ldrx_instance\n",
    "\n",
    "                    ed_kpca.append(ed([a],[b])[0][0])\n",
    "                    cd_kpca.append(1-cd([a],[b])[0][0])\n",
    "\n",
    "                writer.writerow([features,'pca',scope,n,\n",
    "                                             sum(ed_pca)/5, \n",
    "                                             sum(cd_pca)/5, \n",
    "                                             sum(mae_pca)/5])\n",
    "                \n",
    "                writer.writerow([features,'kpca',scope,n,\n",
    "                                             sum(ed_kpca)/5, \n",
    "                                             sum(cd_kpca)/5, \n",
    "                                             (time.time()-ts)/5])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DRex.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lxdr",
   "language": "python",
   "name": "lxdr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
